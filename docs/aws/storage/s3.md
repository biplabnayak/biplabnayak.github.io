# Amazon Simple Storage Service (S3)

[S3 Bucket Properties](#s3-bucket-properties)

* Used for storing files
* Can store huge data
* Limit of single file size in a S3 bucket - Min : 0KB and Max : 5TB
* Object storage, No folder structure
* Its a Regional service. Will have to provide region while creating the S3 bucket
* Durability(probability of maintaining the data without corruption or other damaging) : 99.999999999 (11 9s)
* Availability : between 99.95% to 99.99% (depending on storage class)
* Objects gets copied to multiple location(in different AZ or same AZ based on storage class) to to achieve high availability.

#### To store files in S3:

* Define a bucket - a with a globally unique name (not just unique through the region)
* Bucket names can consist only of lowercase letters, numbers, dots (.), and hyphens (-).
* By default, an account can have 100 bucket. but can request aws to increase the limit
* each object added to the bucket are given a unique key as identifier
* can create folder for organizing data .. S3 is not a file system and all operations are done at bucket level, not folder level

## S3 Storage Classes
 * S3 Standard
 * S3 Intelligent tiering (S3 INT)
 * S3 Standard Infrequent Access (S3 S-IA)
 * S3 1 Zone Infrequent Access (S3 Z-IA)
 * Reduced Redundancy storage (RRS) - Not recommended due to high cost and less durability
 * S3 Glacier
 * S3 Glacier - Deep Archive
 
 ![S3 Storage class Comparision](/images/aws/s3/storage_classes.PNG?raw=true)
 
 
#### S3 Standard
 
 * High throughput, low latency
 * Durability - Eleven 9s
 * Availability SLA - 99.99%
 * Lifecycle Rules  
 ** Lifecycle Rules : they provide an automatic method of managing the life of your data while it is being stored on Amazon S3. By adding a life cycle wall to a bucket you are able to configure and set specific criteria that can automatically move your data from one class to another or delete it from Amazon S3 altogether. You may want to do this as a cost saving exercise by moving data to a cheaper storage class after a set period of time.
 
#### S3 Intelligent tiering
 
 * used when data access Frequency in unknown
 * it has 2 tire - frequent and infrequent tier
 * frequent tier  is expensive and infrequent tier is cheap
 * Object will be initially in frequent tier, but if the object is not accessed in 30 days, object will be autometically moved to infrequent tier.
 If an object is in infrequent tier and and got accessed, it will be moved to frequent tier.
 * Durability - Eleven 9s
 * Availability SLA - 99.9% (less then S3 Standard)
 
#### S3 Standard Infrequent Access (S3 S-IA)

* Similar to infrequent tier of S3 Intelligent tiring
* Cheaper cost then S2 Standard
* Durability - Eleven 9s
* Availability SLA - 99.9% (less then S3 Standard)
 
#### S3 1 Zone Infrequent Access (S3 Z-IA)
 
* Available in single AZ
* Durability - Eleven 9s but across same AZ
* Availability SLA - 99.5% (less then S3 Standard and S3 S-IA)
* 20% lesser cost then standard

#### S3 Glacier

* Low cost long term durable storage solution
* For storing archival data or long term backup
* Very less cost compared to S3 standard
* Does **NOT** provide instant access to data - Takes several hours to get access
* Durability - Eleven 9s
* Data structure are different compared to S3. Buckets and folders does not apply here.
Data structure is set around **vault and archives**. Vaults are acts as a container for the archives.
Archives can be any objects similar to S3.
* The Glacier dashboard within AWS management console allows you to create your vaults, set data retrieval policies, and event notifications. When it comes to moving data into S3 Glacier for the first time it's effectively a two-step process. Firstly, you need to create your vaults as your container for your archives and this could be completed using the Glacier console. Secondly, you need to move your data into the Glacier vault using the available API or SDKs. As you may be thinking, there's also another method of moving your data into Glacier and this is by using the S3 lifecycle rules that I discussed earlier. When it comes to retrieving your archives, which is your data, you will again have to use some form of code to do so, either the APIs, SDKs or the AWS CLI. Either way, you must first create an archival retrieval job, then request access to all or part of that archive.
* Moving data to glacier for thr first time :
  
  * Create your vault - Can use glacier console or API
  * Move data to glacier vault using the API, SDK or CLI
* you can also move data to glacier using S3 life cycle rules

* Retrieving data from glacier :

  * create a archival retrieval job
  * Use API, SDK or CLI to retrieve the data
  * time taken to retrieve is depends on the retrieval option
  
  
* Glacier Retrieval Options :
 
   1. Expedited
     
      * File size : under 200MB
      * Available in 1 to 5 minutes
      * Most expensive retrieval options
   
   2. Standard
   
      * File Size : Any
      * Available in 3 - 5 hours
      * 2nd most expensive retrieval option
      
   3. Bulk
   
      * File Size : can go up-to peta bytes of data
      * Available in 5 - 12 hours
 
* Security :

  1. data encrypted by AES-256 algo
  2. Vault Access Policy
    * resource based policy applied to specific vault
    * max 1 access policy per vault
    * policy contains a "principal" component
  3. Vault Lock policy
    * once set can not be changed
    * used for governance and compliance control
    * eg. put a policy to not delete the data for next 3 years
      
* Pricing : 
  
  1. Single storage cost regardless of how much storage is being used
    
#### S3 Glacier Deep Archive

Out of all the storage classes offered by S3, Glacier Deep Archive is the cheapest and again being a Glacier class, it focuses on long-term storage. This is an ideal storage class for circumstances that require specific data retention regulations and compliance with minimal access, such as those within the financial or health sector where data records might need to be legally retained for seven years or even longer. The durability and availability matches that of S3 Glacier with eleven 9s durability across multiple AZss with 99.9% availability.

* Process of adding data to Glacier Deep Archive is same as S3 glacier
* Retrieval process of Glacier Deep Archive is similar to S3 glacier but **has only one retrieval option which make data available within 12 hours or less**. 

![S3 Glacier Comparision](/images/aws/s3/s3_glacier.PNG?raw=true)

#### S3 Storage Class summery

![S3 Glacier Comparision](/images/aws/s3/s3_summery.PNG?raw=true)

## S3 Bucket Properties

### Versioning

* allows for multiple versions of the same object to exist
* allow you to retrieve previous versions of a file, or recover a file should it subjected to accidental deletion
* managed automatically against objects when you overwrite or delete an object in a bucket that has versioning enabled.
* Versioning is not enabled by default, however, once you have enabled it, you can't disable it, instead, you can only suspend it on the bucket which will prevent any further versions from being created of your objects, but it will keep all existing versions of objects up to the point of suspension. So, there are 3 states in versioning :

   * Unversioned (this is the default state for buckets)
   * Versioning-enabled
   * versioning-suspended

* With versioning enabled, it will consume more storage space(by storing multiple versions of same file) and increase the cost

### Server Access Logging

* server-access logging is enabled on a bucket it captures details of requests that are made to that bucket and its objects
* Its Important for Security, Root Cause Analysis, audits and governance certifications
* guaranteed and is conducted on a best-effort basis by S3. logs are collated and sent every few hours
* Enabling access logging

  * Existing Bucket
    
     * Select Bucket -> Properties Tab -> Server Access Logging tile -> Enable logging (2 fields to complete the step)
     * 1 - Target Bucket - target bucket will be used to store any logs created by enabling server access logging on your source bucket, which must be in the same region
     * 2 - target prefix - add a target prefix that s3 will add to the logs from your source bucket  
    
## Security
1. Bucket Policies 
  * Controls data access
  * constructed as JSON policy

2. Access Control List(ACL)
  * Controls access for users outside account. public access for example
  * permissions like ListObject. WriteObject

3. Data encryption
  * Server side encryption with S3 maneged Keys - SSE-S3
  * Server side encryption with KMS maneged Keys - SSE-KMS
  * Client Side encryption with KMS maneged Keys - CSE-KMS
  * Client Side encryption with Customer maneged Keys - CSE-C
  * SSL Support - secure on transit

## Use cases
### Data Backup
### Static Content & Website
### large dataset
scientific or statistical data. Horizontal scaling of S3 allow multiple parti to analyse data fast - Bigdata analytics
### integration with other AWS service
* eg. EBS snapshots gets stored in S3 (user can not view it in any bucket, managed by aws)
* cloud trail logs stored in S3

## S3 Anti-patterns
* Dynamic/ Fast changing data
* file system requirement
* structured data to be queried
* long term archival storage - use Glacier

## Optimizing S3 performance
### TCP Window Scaling
* When TCP establishes a connection between a source and destination, a three-way handshake takes place which originates from the source (client). Here’s how it works. Before your client can upload an object to S3 a connection to S3 servers must be created. The client will send a TCP packet with a specified TCP window scale factor in the header. This initial TCP request is known as a SYN request, and it’s part one of the three-way handshake. In part two, S3 will receive this request and respond with a SYN/ACK message back to the client with its supported window scale factor. Finally, an ACK message acknowledging the response is sent back to the S3 server. On completion of this three-way handshake, a connection is established and data can be sent between the client and S3. By increasing the window size with a scale factor (window scaling) it enables you to send larger quantities of data in a single segment and therefore more data at a faster rate.
* https://datatracker.ietf.org/doc/html/rfc1323

### TCP Selective Acknowledgement
* Sometimes multiple packets can be lost when using TCP. Understanding which packets have been lost within a TCP window can be difficult to determine. While it’s possible to resend all of the packets, some of these may have already been received by the receiver. TCP selective acknowledgement (SACK) offers a more effective alternative by notifying the sender only in the case of failed packets within that window. As a result, the sender has to resend only the failed packets.
*  RFC-2018

### Scaling S3 Request Rates
* On top of TCP Scaling and TCP SACK communications, S3 itself is already well optimized for very high throughput. In July 2018, AWS announced significant improvements to request rate performance. Prior to this announcement, AWS recommended that users randomize prefixes within buckets to help optimize performance--this is no longer required. Now, you can apply multiple prefixes within buckets to improve request rate performance.You are now able to achieve 3,500 PUT/POST/DELETE requests per second along with 5,500 GET requests. These limitations are based on a single prefix, however there is no limit to the number of prefixes that can be used within an S3 bucket. As a result, if you had 20 prefixes you could reach 70,000 PUT/POST/DELETE and 110,000 GET requests per second within the same bucket.

### Integration of Amazon CloudFront
* Another method to improve optimization is to incorporate Amazon S3 with Amazon CloudFront. This works particularly well if the main request to your S3 data is a GET request. Amazon CloudFront is AWS's content delivery network that speeds up distribution of your static and dynamic content through its worldwide network of edge locations.

## Notes
* S3 Object URL Structure : https://<bucket-name>.s3-<region>.amazonaws.com/<object-prefix>/<object-name> . eg. https://calabs-s3cf-biplab.s3.us-west-2.amazonaws.com/gallery/index.html