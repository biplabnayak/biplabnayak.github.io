## Amazon Simple Storage Service (S3)

* Used for storing files
* Can store huge data
* Limit of single file size in a S3 bucket - Min : 0KB and Max : 5TB
* Object storage, No folder structure
* Its a Regional service. Will have to provide region while creating the S3 bucket
* Durability(probability of maintaining the data without corruption or other damaging) : 99.999999999 (11 9s)
* Availability : between 99.95% to 99.99% (depending on storage class)
* Objects gets copied to multiple location(in different AZ or same AZ based on storage class) to to achieve high availability.

#### To store files in S3:
* Define a bucket - a with a globally unique name (not just unique through the region)
* By default, an account can have 100 bucket. but can request aws to increase the limit
* each object added to the bucket are given a unique key as identifier
* can create folder for organizing data .. S3 is not a file system and all operations are done at bucket level, not folder level

 ### S3 Storage Classes
 * S3 Standard
 * S3 Intelligent tiering (S3 INT)
 * S3 Standard Infrequent Access (S3 S-IA)
 * S3 1 Zone Infrequent Access (S3 Z-IA)
 * S3 Glacier
 * S3 Glacier - Deep Archive
 
 ![S3 Storage class Comparision](/images/aws/s3/storage_classes.PNG?raw=true)
 
 ##### S3 Standard
 * High throughput, low latency
 * Durability - Eleven 9s
 * Availability SLA - 99.99%
 * Lifecycle Rules  
 ** Lifecycle Rules : they provide an automatic method of managing the life of your data while it is being stored on Amazon S3. By adding a life cycle wall to a bucket you are able to configure and set specific criteria that can automatically move your data from one class to another or delete it from Amazon S3 altogether. You may want to do this as a cost saving exercise by moving data to a cheaper storage class after a set period of time.
 ##### S3 Intelligent tiring
 * used when data access Frequency in unknown
 * it has 2 tire - frequent and infrequent tier
 * frequent tier  is expensive and infrequent tier is cheap
 * Object will be initially in frequent tier, but if the object is not accessed in 30 days, object will be autometically moved to infrequent tier.
 If an object is in infrequent tier and and got accessed, it will be moved to frequent tier.
 * Durability - Eleven 9s
 * Availability SLA - 99.9% (less then S3 Standard)
 
##### S3 Standard Infrequent Access (S3 S-IA)
* Similar to infrequent tier of S3 Intelligent tiring
* Cheaper cost then S2 Standard
* Durability - Eleven 9s
* Availability SLA - 99.9% (less then S3 Standard)
 
 ##### S3 1 Zone Infrequent Access (S3 Z-IA)
* Available in single AZ
* Durability - Eleven 9s but across same AZ
* Availability SLA - 99.5% (less then S3 Standard and S3 S-IA)
* 20% lesser cost then standard

### S3 Glacier

##### S3 Glacier
* Low cost long term durable storage solution
* For storing archival data or long term backup
* Very less cost compared to S3 standard
* Does **NOT** provide instant access to data - Takes several hours to get access
* Durability - Eleven 9s
* Data structure are different compared to S3. Buckets and folders does not apply here.
Data structure is set around **vault and archives**. Vaults are acts as a container for the archives.
Archives can be any objects similar to S3.
* The Glacier dashboard within AWS management console allows you to create your vaults, set data retrieval policies, and event notifications. When it comes to moving data into S3 Glacier for the first time it's effectively a two-step process. Firstly, you need to create your vaults as your container for your archives and this could be completed using the Glacier console. Secondly, you need to move your data into the Glacier vault using the available API or SDKs. As you may be thinking, there's also another method of moving your data into Glacier and this is by using the S3 lifecycle rules that I discussed earlier. When it comes to retrieving your archives, which is your data, you will again have to use some form of code to do so, either the APIs, SDKs or the AWS CLI. Either way, you must first create an archival retrieval job, then request access to all or part of that archive.
* Moving data to glacier for thr first time :
  
  * Create your vault - Can use glacier console or API
  * Move data to glacier vault using the API, SDK or CLI
* you can also move data to glacier using S3 life cycle rules

* Retrieving data from glacier :

  * create a archival retrieval job
  * Use API, SDK or CLI to retrieve the data
  * time taken to retrieve is depends on the retrieval option
  
  
* Glacier Retrieval Options :
 
   1. Expedited
     
      * File size : under 200MB
      * Available in 1 to 5 minutes
      * Most expensive retrieval options
   
   2. Standard
   
      * File Size : Any
      * Available in 3 - 5 hours
      * 2nd most expensive retrieval option
      
   3. Bulk
   
      * File Size : can go up-to peta bytes of data
      * Available in 5 - 12 hours
      
      
##### S3 Glacier Deep Archive

Out of all the storage classes offered by S3, Glacier Deep Archive is the cheapest and again being a Glacier class, it focuses on long-term storage. This is an ideal storage class for circumstances that require specific data retention regulations and compliance with minimal access, such as those within the financial or health sector where data records might need to be legally retained for seven years or even longer. The durability and availability matches that of S3 Glacier with eleven 9s durability across multiple AZss with 99.9% availability.

* Process of adding data to Glacier Deep Archive is same as S3 glacier
* Retrieval process of Glacier Deep Archive is similar to S3 glacier but **has only one retrieval option which make data available within 12 hours or less**. 

![S3 Glacier Comparision](/images/aws/s3/s3_glacier.PNG?raw=true)

##### S3 Storage Class summery

![S3 Glacier Comparision](/images/aws/s3/s3_summery.PNG?raw=true)